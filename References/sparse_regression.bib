@Manual{rpart,
    title = {rpart: Recursive Partitioning and Regression Trees},
    author = {Therneau, T. and  Atkinson, B.},
    year = {2019},
    note = {R package version 4.1-15},
    url = {https://CRAN.R-project.org/package=rpart},
  }


@Article{ranger,
    title = {{ranger}: A Fast Implementation of Random Forests for High Dimensional Data in {C++} and {R}},
    author = {Wright, M. N.  and Ziegler, A.},
    journal = {Journal of Statistical Software},
    year = {2017},
    volume = {77},
    number = {1},
    pages = {1--17},
    doi = {10.18637/jss.v077.i01},
  }


@article {SL2007,
    AUTHOR = {van der Laan, M. J. and Polley, E. C. and Hubbard, A.
              E.},
     TITLE = {Super learner},
   JOURNAL = {Stat. Appl. Genet. Mol. Biol.},
  FJOURNAL = {Statistical Applications in Genetics and Molecular Biology},
    VOLUME = {6},
      YEAR = {2007},
     PAGES = {Art. 25, 23},
      optISSN = {1544-6115},
   optMRCLASS = {Expansion},
  optMRNUMBER = {2349918},
       optDOI = {10.2202/1544-6115.1309},
       optURL = {https://doi.org/10.2202/1544-6115.1309},
}

@incollection {SLchapter,
    AUTHOR = {Polley, E. C. and Rose, S. and van der Laan, M. J.},
     TITLE = {Super learning},
 BOOKTITLE = {Targeted learning},
    SERIES = {Springer Ser. Statist.},
     PAGES = {43--66},
 PUBLISHER = {Springer, New York},
      YEAR = {2011},
   optMRCLASS = {62G05 (62-09 62F10 62J99)},
  optMRNUMBER = {2867115},
       optDOI = {10.1007/978-1-4419-9782-1\_3},
       optURL = {https://doi.org/10.1007/978-1-4419-9782-1_3},
}

@Manual{SuperLearner,
    title = {SuperLearner: Super Learner Prediction},
    author = {Polley, E. and  LeDell, E. and  Kennedy, C. and  {van der Laan}, M. J.},
    year = {2021},
    note = {R package version 2.0-28},
    url = {https://CRAN.R-project.org/package=SuperLearner},
}


@book {bochnak98,
    AUTHOR = {Bochnak, J. and Coste, M. and Roy, M-F.},
     TITLE = {Real algebraic geometry},
    SERIES = {Ergebnisse der Mathematik und ihrer Grenzgebiete (3) [Results
              in Mathematics and Related Areas (3)]},
    VOLUME = {36},
      optNOTE = {Translated from the 1987 French original,
              Revised by the authors},
 PUBLISHER = {Springer-Verlag, Berlin},
      YEAR = {1998},
     optPAGES = {x+430},
      optISBN = {3-540-64663-9},
   optMRCLASS = {14Pxx (11E25 32C05 58A07)},
  optMRNUMBER = {1659509},
optMRREVIEWER = {A. Tognoli},
       optDOI = {10.1007/978-3-662-03718-8},
       optURL = {https://doi.org/10.1007/978-3-662-03718-8},
}

@book {benedetti90,
    AUTHOR = {Benedetti, R. and Risler, J-J.},
     TITLE = {Real algebraic and semi-algebraic sets},
    SERIES = {Actualit\'{e}s Math\'{e}matiques. [Current Mathematical Topics]},
 PUBLISHER = {Hermann, Paris},
      YEAR = {1990},
     optPAGES = {340},
      optISBN = {2-7056-6144-1},
   optMRCLASS = {14P05 (12D10 14P10)},
  optMRNUMBER = {1070358},
optMRREVIEWER = {Robert Silhol},
}
	

@InProceedings{pmlr-v32-cuturi14,
  title = 	 {Fast Computation of {W}asserstein Barycenters},
  author = 	 {Cuturi, M. and Doucet, A.},
  booktitle = 	 {Proceedings of the 31st International Conference on Machine Learning},
  pages = 	 {685--693},
  year = 	 {2014},
  editor = 	 {Xing, E. P. and Jebara, T.},
  volume = 	 {32},
  optnumber =       {2},
  series = 	 {Proceedings of Machine Learning Research},
  optaddress = 	 {Bejing, China},
  month = 	 {22--24 Jun},
  publisher =    {PMLR},
  optpdf = 	 {http://proceedings.mlr.press/v32/cuturi14.pdf},
  opturl = 	 {https://proceedings.mlr.press/v32/cuturi14.html},
  optabstract = 	 {We present new algorithms to compute the mean of a set of $N$ empirical probability measures under the optimal transport metric. This mean, known as the Wasserstein barycenter&nbsp;(Agueh and Carlier, 2011; Rabin et al, 2012), is the measure that minimizes the sum of its Wasserstein distances to each element in that set. We argue through a simple example that Wasserstein barycenters have appealing properties that differentiate them from other barycenters proposed recently, which all build on kernel smoothing and/or Bregman divergences. Two original algorithms are proposed that require the repeated computation of primal and dual optimal solutions of transport problems. However direct implementation of these algorithms is too costly as optimal transports are notoriously computationally expensive. Extending the work of Cuturi (2013), we smooth both the primal and dual of the optimal transport problem to recover fast approximations of the primal and dual optimal solutions. We apply these algorithms to the visualization of perturbed images and to a clustering problem.}
}


@Article{Charpentier_2021,
  author = 	 {Charpentier, A. and James, M. and Ali, H.},
  title = 	 {Predicting Drought and Subsidence Risks in {F}rance},
  journal = 	 {Natural Hazards and Earth System Sciences Discussion},
  year = 	 {2021},
  OPTkey = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTpages = 	 {},
  OPTmonth = 	 {},
  note = 	 {In review},
  doi = {10.5194/nhess-2021-214},
  OPTannote = 	 {}
}

@TechReport{osasl1,
  author = 	 {Ecoto, G. and Bibaut, A. F. and Chambaz, A.},
  title = 	 {One-step ahead sequential {S}uper {L}earning from short times series of many slightly dependent data, and anticipating the cost of natural disasters},  
  OPTinstitution =  {},
  year = 	 {2021},
  OPTkey = 	 {},
  OPTtype = 	 {},
  OPTnumber = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  note = 	 {Submitted},
  url = {https://arxiv.org/abs/2107.13291},
  OPTannote = 	 {}
}

@TechReport{osasl2,
  author = 	 {Ecoto, G.  and Chambaz, A.},
  title = 	 {Anticipating the cost of drought events in {F}rance by super learning},  
  OPTinstitution =  {},
  year = 	 {2022},
  OPTkey = 	 {},
  OPTtype = 	 {},
  OPTnumber = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  note = 	 {Submitted},
  url = {https://arxiv.org/abs/2206.11545},
  OPTannote = 	 {}
}


@misc{kaiser2021differentiability,
      title={Differentiability Properties of Log-Analytic Functions}, 
      author={Kaiser, T. and Opris, A. },
      year={2021},
      eprint={2007.03332},
      archivePrefix={arXiv},
      primaryClass={math.LO}
}

@inproceedings{Coste2002ANIT,
  title={An introduction to semialgebraic geometry},
  author={Coste, M.},
  year={2002}
}

@article {attouch2013proximal,
    AUTHOR = {Attouch, H. and Bolte, J. and Redont, P. and              Soubeyran, A.},
     TITLE = {Proximal alternating minimization and projection methods for
              nonconvex problems: an approach based on the
              {K}urdyka-{L}ojasiewicz inequality},
   JOURNAL = {Math. Oper. Res.},
  optFJOURNAL = {Mathematics of Operations Research},
    VOLUME = {35},
      YEAR = {2010},
    NUMBER = {2},
     PAGES = {438--457},
      optISSN = {0364-765X},
   optMRCLASS = {90C26 (49J52 65K10)},
  optMRNUMBER = {2674728},
       optDOI = {10.1287/moor.1100.0449},
       optURL = {https://doi.org/10.1287/moor.1100.0449},
}
		


@ARTICLE{Wilkie96modelcompleteness,
    author = {Wilkie, A. J. },
    title = {Model completeness results for expansions of the ordered field of real numbers by restricted {P}faffian functions and the exponential function},
    journal = {J. Amer. Math. Soc},
    year = {1996},
    volume = {9},
    pages = {1051--1094}
}



@article {doi:10.1137/060670080,
    AUTHOR = {Bolte, J. and Daniilidis, A. and Lewis, A. and               Shiota, M.},
     TITLE = {Clarke subgradients of stratifiable functions},
   JOURNAL = {SIAM J. Optim.},
  optFJOURNAL = {SIAM Journal on Optimization},
    VOLUME = {18},
      YEAR = {2007},
    NUMBER = {2},
     PAGES = {556--572},
     optISSN = {1052-6234},
   optMRCLASS = {49J52 (26D10 32B20)},
  optMRNUMBER = {2338451},
optMRREVIEWER = {Marian Mure\c{s}an},
       optDOI = {10.1137/060670080},
       optURL = {https://doi.org/10.1137/060670080},
}
	

@article {iPiano,
    AUTHOR = {Ochs, P. and Brox, T. and Pock, T.},
     TITLE = {i{P}iano: inertial proximal algorithm for strongly convex
              optimization},
   JOURNAL = {J. Math. Imaging Vision},
  optFJOURNAL = {Journal of Mathematical Imaging and Vision},
    VOLUME = {53},
      YEAR = {2015},
    NUMBER = {2},
     PAGES = {171--181},
      optISSN = {0924-9907},
   optMRCLASS = {94A08},
  optMRNUMBER = {3372139},
       optDOI = {10.1007/s10851-015-0565-0},
       optURL = {https://doi.org/10.1007/s10851-015-0565-0},
}
	

@misc{peyre2020computational,
      title={Computational Optimal Transport}, 
      author={ Peyré, G. and  Cuturi, M.},
      year={2020},
      eprint={1803.00567},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}


@inproceedings{NIPS2013_af21d0c9,
 author = {Cuturi, M.},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Burges, C. J. C.  and Bottou, L.  and  Welling, M. and Ghahramani, Z. and Weinberger, K. Q. },
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {{S}inkhorn Distances: Lightspeed Computation of Optimal Transport},
 opturl = {https://proceedings.neurips.cc/paper/2013/file/af21d0c97db2e27e13572cbf59eb343d-Paper.pdf},
 volume = {26},
 year = {2013}
}


@InProceedings{genevay2017learning,
  title = 	 {Learning Generative Models with {S}inkhorn Divergences},
  author = 	 {Genevay, A. and Peyré, G. and Cuturi, M.},
  booktitle = 	 {Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1608--1617},
  year = 	 {2018},
  editor = 	 {Storkey, Amos and Perez-Cruz, Fernando},
  volume = 	 {84},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--11 Apr},
  publisher =    {PMLR},
  optpdf = 	 {http://proceedings.mlr.press/v84/genevay18a/genevay18a.pdf},
  opturl = 	 {https://proceedings.mlr.press/v84/genevay18a.html},
  optabstract = 	 {The ability to compare two degenerate probability distributions, that is two distributions supported on low-dimensional manifolds in much higher-dimensional spaces, is a crucial factor in the estimation of generative mod- els.It is therefore no surprise that optimal transport (OT) metrics and their ability to handle measures with non-overlapping sup- ports have emerged as a promising tool. Yet, training generative machines using OT raises formidable computational and statistical challenges, because of (i) the computational bur- den of evaluating OT losses, (ii) their instability and lack of smoothness, (iii) the difficulty to estimate them, as well as their gradients, in high dimension. This paper presents the first tractable method to train large scale generative models using an OT-based loss called Sinkhorn loss which tackles these three issues by relying on two key ideas: (a) entropic smoothing, which turns the original OT loss into a differentiable and more robust quantity that can be computed using Sinkhorn fixed point iterations; (b) algorithmic (automatic) differentiation of these iterations with seam- less GPU execution. Additionally, Entropic smoothing generates a family of losses interpolating between Wasserstein (OT) and Energy distance/Maximum Mean Discrepancy (MMD) losses, thus allowing to find a sweet spot leveraging the geometry of OT on the one hand, and the favorable high-dimensional sample complexity of MMD, which comes with un- biased gradient estimates. The resulting computational architecture complements nicely standard deep network generative models by a stack of extra layers implementing the loss function.}
}


@InProceedings{feydy2018interpolating,
  title = 	 {Interpolating between Optimal Transport and {MMD} using {S}inkhorn Divergences},
  author =       {Feydy, J. and S\'{e}journ\'{e}, T. and Vialard, F-X. and Amari, S. and Trouv\'{e}, A. and Peyr\'{e}, G.},
  booktitle = 	 {Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics},
  pages = 	 {2681--2690},
  year = 	 {2019},
  editor = 	 {Chaudhuri, K. and Sugiyama, M.},
  volume = 	 {89},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {16--18 Apr},
  publisher =    {PMLR},
  optpdf = 	 {http://proceedings.mlr.press/v89/feydy19a/feydy19a.pdf},
  opturl = 	 {https://proceedings.mlr.press/v89/feydy19a.html},
  optabstract = 	 {Comparing probability distributions is a fundamental problem in data sciences. Simple norms and divergences such as the total variation and the relative entropy only compare densities in a point-wise manner and fail to capture the geometric nature of the problem. In sharp contrast, Maximum Mean Discrepancies (MMD) and Optimal Transport distances (OT) are two classes of distances between measures that take into account the geometry of the underlying space and metrize the convergence in law.  This paper studies the Sinkhorn divergences, a family of geometric divergences that interpolates between MMD and OT. Relying on a new notion of geometric entropy, we provide theoretical guarantees for these divergences: positivity, convexity and metrization of the convergence in law. On the practical side, we detail a numerical scheme that enables the large scale application of these divergences for machine learning: on the GPU, gradients of the Sinkhorn loss can be computed for batches of a million samples.}
}


@book {RockWets98,
    AUTHOR = {Rockafellar, R. T. and Wets, R. J.-B.},
     TITLE = {Variational analysis},
    SERIES = {Grundlehren der mathematischen Wissenschaften [Fundamental
              Principles of Mathematical Sciences]},
    VOLUME = {317},
 PUBLISHER = {Springer-Verlag, Berlin},
      YEAR = {1998},
     optPAGES = {xiv+733},
      optISBN = {3-540-62772-3},
   optMRCLASS = {49-02 (46N10 47N10 49J52 49K40 90C30)},
  optMRNUMBER = {1491362},
optMRREVIEWER = {Francis H. Clarke},
       optDOI = {10.1007/978-3-642-02431-3},
       optURL = {https://doi.org/10.1007/978-3-642-02431-3},
}


@article {Franklin,
    AUTHOR = {Franklin, J. and Lorenz, J.},
     TITLE = {On the scaling of multidimensional matrices},
   JOURNAL = {Linear Algebra Appl.},
  FJOURNAL = {Linear Algebra and its Applications},
    VOLUME = {114/115},
      YEAR = {1989},
     optPAGES = {717--735},
      optISSN = {0024-3795},
   optMRCLASS = {15A99},
  optMRNUMBER = {986904},
optMRREVIEWER = {D. S. Tracy},
       optDOI = {10.1016/0024-3795(89)90490-4},
       optURL = {https://doi.org/10.1016/0024-3795(89)90490-4},
}

@article {Birkhoff,
    AUTHOR = {Birkhoff, G.},
     TITLE = {Extensions of {J}entzsch's theorem},
   JOURNAL = {Trans. Amer. Math. Soc.},
  FJOURNAL = {Transactions of the American Mathematical Society},
    VOLUME = {85},
      YEAR = {1957},
     optPAGES = {219--227},
      optISSN = {0002-9947},
   optMRCLASS = {46.2X},
  optMRNUMBER = {87058},
optMRREVIEWER = {E. H. Rothe},
       optDOI = {10.2307/1992971},
       optURL = {https://doi.org/10.2307/1992971},
}


@article {Danskin,
    AUTHOR = {Danskin, J. M.},
     TITLE = {The theory of {${\rm max-min}$}, with applications},
   JOURNAL = {SIAM J. Appl. Math.},
  FJOURNAL = {SIAM Journal on Applied Mathematics},
    VOLUME = {14},
      YEAR = {1966},
     optPAGES = {641--664},
      optISSN = {0036-1399},
   optMRCLASS = {90.58},
  optMRNUMBER = {210456},
optMRREVIEWER = {L. D. Berkovitz},
       optDOI = {10.1137/0114053},
       optURL = {https://doi.org/10.1137/0114053},
}

@book{Bertsekas99,
    AUTHOR = {Bertsekas, D. P.},
     TITLE = {Nonlinear programming},
    SERIES = {Athena Scientific Optimization and Computation Series},
   optEDITION = {Second},
 PUBLISHER = {Athena Scientific, Belmont, MA},
      YEAR = {1999},
     optPAGES = {xiv+777},
      optISBN = {1-886529-00-0},
   optMRCLASS = {49-01 (90-01)},
  optMRNUMBER = {3444832},
}


@inproceedings{L1-ball,
author = {Duchi, J. and Shalev-Shwartz, S. and Singer, Y. and Chandra, T.},
title = {Efficient Projections onto the l1-Ball for Learning in High Dimensions},
year = {2008},
optisbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
opturl = {https://doi.org/10.1145/1390156.1390191},
optdoi = {10.1145/1390156.1390191},
optabstract = {We describe efficient algorithms for projecting a vector onto the l1-ball. We present two methods for projection. The first performs exact projection in O(n) expected time, where n is the dimension of the space. The second works on vectors k of whose elements are perturbed outside the l1-ball, projecting in O(k log(n)) time. This setting is especially useful for online learning in sparse feature spaces such as text categorization applications. We demonstrate the merits and effectiveness of our algorithms in numerous batch and online learning tasks. We show that variants of stochastic gradient projection methods augmented with our efficient projection procedures outperform interior point methods, which are considered state-of-the-art optimization techniques. We also show that in online settings gradient updates with l1 projections outperform the exponentiated gradient algorithm while obtaining models with high degrees of sparsity.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {272--279},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

  

